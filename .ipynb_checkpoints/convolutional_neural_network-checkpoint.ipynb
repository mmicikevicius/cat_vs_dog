{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3DR-eO17geWu"
   },
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EMefrVPCg-60"
   },
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sCV30xyVhFbE"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import imghdr\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oxQxCBWyoGPE"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MvE-heJNo3GG"
   },
   "source": [
    "### Preprocessing the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0koUcJMJpEBD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   rotation_range = 270,\n",
    "                                   vertical_flip = True)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('dataset/training_set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mrCMmGw9pHys"
   },
   "source": [
    "### Preprocessing the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SH4WzfOhpKc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "af8O4l90gk7B"
   },
   "source": [
    "## Building the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model():\n",
    "    cnn = tf.keras.models.Sequential() #Initialising the CNN\n",
    "    cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3])) #Convolution\n",
    "    cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2)) #Pooling\n",
    "    cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu')) #Adding a second convolutional layer\n",
    "    cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2)) #Second Layer Pooling\n",
    "    cnn.add(tf.keras.layers.Flatten()) #Flattening\n",
    "    cnn.add(tf.keras.layers.Dense(units=128, activation='relu')) #Full Connection\n",
    "    cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid')) #Output Layer\n",
    "    cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy']) #Compiling the CNN\n",
    "    return cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D6XkI90snSDl"
   },
   "source": [
    "## Training the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ehS-v3MIpX2h"
   },
   "source": [
    "### Training the CNN on the Training set and evaluating it on the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XUj1W4PJptta"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 50s 199ms/step - loss: 0.6984 - accuracy: 0.5249 - val_loss: 0.6721 - val_accuracy: 0.6060\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 37s 147ms/step - loss: 0.6552 - accuracy: 0.6188 - val_loss: 0.6423 - val_accuracy: 0.6365\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 41s 165ms/step - loss: 0.6324 - accuracy: 0.6418 - val_loss: 0.6572 - val_accuracy: 0.6530\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 0.6087 - accuracy: 0.6669 - val_loss: 0.5798 - val_accuracy: 0.7025\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 44s 174ms/step - loss: 0.6138 - accuracy: 0.6627 - val_loss: 0.5937 - val_accuracy: 0.6790\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 43s 174ms/step - loss: 0.5951 - accuracy: 0.6769 - val_loss: 0.5983 - val_accuracy: 0.6750\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 43s 174ms/step - loss: 0.5858 - accuracy: 0.6854 - val_loss: 0.5626 - val_accuracy: 0.7180\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 43s 173ms/step - loss: 0.5859 - accuracy: 0.6838 - val_loss: 0.5401 - val_accuracy: 0.7185\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 43s 173ms/step - loss: 0.5712 - accuracy: 0.7018 - val_loss: 0.5628 - val_accuracy: 0.7030\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 43s 172ms/step - loss: 0.5584 - accuracy: 0.7012 - val_loss: 0.5516 - val_accuracy: 0.7185\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 43s 174ms/step - loss: 0.5673 - accuracy: 0.7035 - val_loss: 0.5280 - val_accuracy: 0.7420\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 43s 173ms/step - loss: 0.5653 - accuracy: 0.7079 - val_loss: 0.5886 - val_accuracy: 0.6950\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 42s 168ms/step - loss: 0.5620 - accuracy: 0.7067 - val_loss: 0.5207 - val_accuracy: 0.7500\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 42s 166ms/step - loss: 0.5530 - accuracy: 0.7158 - val_loss: 0.5810 - val_accuracy: 0.7055\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 41s 165ms/step - loss: 0.5490 - accuracy: 0.7196 - val_loss: 0.5203 - val_accuracy: 0.7350\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 41s 165ms/step - loss: 0.5326 - accuracy: 0.7290 - val_loss: 0.5118 - val_accuracy: 0.7380\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 41s 165ms/step - loss: 0.5561 - accuracy: 0.7141 - val_loss: 0.5341 - val_accuracy: 0.7295\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.5297 - accuracy: 0.7306 - val_loss: 0.4976 - val_accuracy: 0.7530\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 41s 164ms/step - loss: 0.5423 - accuracy: 0.7199 - val_loss: 0.5379 - val_accuracy: 0.7320\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 42s 166ms/step - loss: 0.5366 - accuracy: 0.7286 - val_loss: 0.5606 - val_accuracy: 0.7215\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 41s 164ms/step - loss: 0.5229 - accuracy: 0.7326 - val_loss: 0.5448 - val_accuracy: 0.7235\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 41s 165ms/step - loss: 0.5229 - accuracy: 0.7359 - val_loss: 0.5316 - val_accuracy: 0.7290\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 41s 164ms/step - loss: 0.5226 - accuracy: 0.7400 - val_loss: 0.5227 - val_accuracy: 0.7385\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 42s 166ms/step - loss: 0.5180 - accuracy: 0.7416 - val_loss: 0.4980 - val_accuracy: 0.7605\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 41s 164ms/step - loss: 0.5273 - accuracy: 0.7277 - val_loss: 0.5157 - val_accuracy: 0.7405\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9ca2e3dfd0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_results = cnn_model()\n",
    "cnn_results.fit(x = training_set, validation_data = test_set, epochs = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.32700673],\n",
       "       [0.06832469],\n",
       "       [0.27782148],\n",
       "       ...,\n",
       "       [0.9983927 ],\n",
       "       [0.8219224 ],\n",
       "       [0.8939076 ]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_results.predict(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U3PZasO0006Z"
   },
   "source": [
    "## Making new predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gsSiWEJY1BPB"
   },
   "outputs": [],
   "source": [
    "def cat_dog_prediction(directory = 'dataset/single_prediction'):\n",
    "    rows = []\n",
    "    for f in os.listdir(directory):\n",
    "        file_type = imghdr.what(directory + '/' + f)\n",
    "        if file_type in ('png', 'jpeg'):\n",
    "            test_image = image.load_img(directory + '/' + f, target_size = (64, 64))\n",
    "            test_image = image.img_to_array(test_image)\n",
    "            test_image = np.expand_dims(test_image, axis = 0)\n",
    "            result = cnn_results.predict(test_image)\n",
    "            training_set.class_indices\n",
    "            if result[0][0] == 1:\n",
    "              prediction = 'dog'\n",
    "            else:\n",
    "              prediction = 'cat'\n",
    "        else:\n",
    "            prediction = 'unsupported_file'\n",
    "        rows.append([f, prediction])\n",
    "    results_table = pd.DataFrame(rows, columns=[\"Image Name\", \"Prediction\"])\n",
    "    print(results_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Image Name        Prediction\n",
      "0     dog1.jpg               dog\n",
      "1     cat1.jpg               dog\n",
      "2     cat2.png               dog\n",
      "3  2150942.pdf  unsupported_file\n"
     ]
    }
   ],
   "source": [
    "cat_dog_prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ImageDataGenerator' object has no attribute 'image'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-1f8ca87d5069>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtestt_datagen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrescale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtestt_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestt_datagen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'ImageDataGenerator' object has no attribute 'image'"
     ]
    }
   ],
   "source": [
    "testt_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "testt_set = testt_datagen.image.load_img(directory + '/' + f, target_size = (64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "convolutional_neural_network.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
